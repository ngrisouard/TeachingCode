{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Supporting textbook chapters for week 3: 5.5, 5.6, 5.10*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lecture 3, topics:**\n",
    "\n",
    "* Generalization of trapz and Simpson: Newton-Cotes formulas\n",
    "* Integration by Gaussian quadrature\n",
    "* Numerical differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical integration\n",
    "\n",
    "* Think of integrals as areas under curves.\n",
    "* Approximate these areas in terms of simple shapes (rectangles, trapezoids, rectangles with parabolic tops)\n",
    "\n",
    "![From Newman, composite of figs. 5.1 and 5.2.](RecTrapSimp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Riemann sum: approximate function over each slice as a constant ($0^{\\text{th}}$-order polynomial)\n",
    "* Trapezoidal rule: approximate function over each slice as a straight line ($1^\\text{st}$-order polynomial)\n",
    "* Simpson's rule: approximate function over each two adjacent slices as a parabola ($2^\\text{nd}$-order polynomial)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Newton-Cotes formulas\n",
    "\n",
    "### General idea\n",
    "\n",
    "Trapezoid and Simpson's Rules are part of a more general set of integration rules:\n",
    "* Break your interval into small **equal** sub-intervals,\n",
    "* approximate your function by a polynomial of some degree, e.g. \n",
    "    * 0 for mid-point rule (that's just summing all elements and multiplying by $h$)\n",
    "    * 1 for Trapz,\n",
    "    * 2 for Simpson\n",
    "on that sub-interval.\n",
    "* this class of methods leads to Newton-Cotes (N-C) formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* All Newton-Cotes formulas can be written in the form:\n",
    "$$\\int_a^b f(x) dx \\approx \\sum_{k=1}^{N+1} w_k f(x_k).$$\n",
    "* $w_k$: \"weights\".\n",
    "* $x_k$: \"sample points\". Notice above we are using $N+1$ points ($N$ slices) to sample.\n",
    "* N-C formulas of degree $N$: exact for polynomials of degree $N$ (which require $N+1$ points to determine)\n",
    "* For N-C formulas, the sample points are **evenly spaced**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "* All Newton-Cotes formulas can be written in the form:\n",
    "$$I(a, b) = \\int_a^b f(x) dx \\approx \\sum_{k=1}^{N+1} w_k f(x_k).$$\n",
    "\n",
    "**Example: Riemann sum**\n",
    "\n",
    "$$I(a, b) \\approx h \\sum_{n=1}^{N} f(a+nh).$$\n",
    "\n",
    "* weights: $w_k = h$ except $w_{N+1}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example: trapezoidal rule**\n",
    "$$I(a, b) \\approx h\\left[\\frac12 f(a) + \\frac12f(b) + \\sum_{k=2}^{N} f(a+kh)\\right].$$\n",
    "\n",
    "* weights:\n",
    "    * $w_k = h/2$ for $k=1$ or $N+1$,\n",
    "    * $w_k = h$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All Newton-Cotes formulas can be written in the form:\n",
    "$$I(a, b) = \\int_a^b f(x) dx \\approx \\sum_{k=1}^{N+1} w_k f(x_k).$$\n",
    "\n",
    "**Example: Simpson's rule**\n",
    "$$I(a,b) \\approx \\frac{h}3\\left[f(a) + f(b) + 4\\sum_{\\substack{k\\ odd\\\\ 3\\dots{}N}}f(a+kh) + 2\\sum_{\\substack{k\\ even \\\\ 2\\dots{}N-1}}f(a+kh)\\right].$$\n",
    "\n",
    "weights:\n",
    "* For $k=1$ or $N+1$: $w_k = h/3$ \n",
    "* For $k=2$, $4$,$\\dots{}$, $N$ (recall: $N$ even): $w_k = 2h/3$ \n",
    "* For $k=3$, $5$,$\\dots{}$, $N-1$: $w_k = 4h/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalization\n",
    "\n",
    "| Degree | Shape | $k=1$ | $2$ | $3$ | $4$ | $5$ | $6$ | $7$ | $\\dots$ | $N$ | $N+1$ |  \n",
    "| --: | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| 1 (trapezoidal) | Straight line | $1/2$ | $1$ | $1$ | $\\dots$ | $\\dots$ | $\\dots$ | $\\dots$ | $\\dots$ | $1$ | $1/2$ |\n",
    "| 2 (Simpson) | Parabola | $1/3$ | $4/3$| $2/3$ | $4/3$ | $\\dots$ | $\\dots$ | $\\dots$ | $\\dots$ | $4/3$ | $1/3$ |\n",
    "| 3 | Cubic | $3/8$ | $9/8$ | $9/8$ | $3/4$ | $9/8$ | $9/8$ | $3/4$ | $\\dots$ | $9/8$ | $3/8$ |\n",
    "| 4 | Quartic | $14/45$ | $64/45$ | $8/15$ | $64/45$ | $28/45$ | $64/45$ | $8/15$ | $\\dots$ | $64/45$ | $14/45$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian quadrature\n",
    "\n",
    "### Presentation\n",
    "\n",
    "Newton-Cotes:\n",
    "* had to use $N$ equally-spaced sampled points.\n",
    "* $N^{\\text{th}}$-order N-C exact for polynomial of degree $N$.\n",
    "* A $N^{\\text{th}}$-order polynomial approximates a well-behaved function better than a $(N-1)^{\\text{th}}$-order polynomial, because of the added \"knob\" one can turn.\n",
    "\n",
    "Gaussian quadrature:\n",
    "* $N$ unequally-spaced points $\\Rightarrow$ $N$ more \"knobs\" to turn,\n",
    "* exact for $(2N-1)^{\\text{th}}$-order polynomial.\n",
    "* other way to look at it: it will give the same level of accuracy as an approximation by a $(2N-1)^{\\text{th}}$-order polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remarkably, there is a universal rule to choose the $w_k$ and $x_k$:\n",
    "* $x_k = $ roots of $N^\\text{th}$ Legendre polynomial $P_N(x)$.\n",
    "* $\\displaystyle w_k = \\left[\\frac{2}{1-x^2}\\left(\\frac{dP_N}{dx}\\right)^{-2}\\right]_{x={x_k}}$, while $P_N(x_k)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Legendre polynomials: cool set of functions.\n",
    "* Defined to be mutually orthogonal:\n",
    "    $$\\forall (M, N) \\in\\mathbb N^2, \\quad \\int_{-1}^1 P_N(x)P_M(x) dx = \\frac{2\\delta_{MN}}{2N+1}.$$\n",
    "    $\\delta_{ij}$ is the Kronecker delta, $\\delta_{ij} = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Defined recursively:\n",
    "    $$P_0(x) = 1 \\Rightarrow P_1(x) = x \\Rightarrow P_2(x) = \\dots,\\ \\text{and}$$\n",
    "    \n",
    "    $$(N+1)P_{N+1}(x) = (2N+1)xP_N(x) -NP_{N-1}(x),\\ \\text{or}$$\n",
    "    \n",
    "    $$\\frac{d}{dx}\\left[(1-x^2)\\frac{d P_{N+1}}{dx}(x)\\right] = -N(N+1)P_N(x),\\ \\text{or}$$\n",
    "    \n",
    "    $$P_N(x) = \\frac1{2^N N!}\\frac{d^N}{dx^N}\\left[(x^2-1)^N\\right]\\dots$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import legendre\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace\n",
    "\n",
    "def plot_legendre(up_to_order):\n",
    "    x = linspace(-1, 1, 128)\n",
    "    plt.figure(dpi=150)\n",
    "    for N in range(up_to_order+1):\n",
    "        plt.plot(x, legendre(N)(x), label='$N = {}$'.format(N))\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$P_N(x)$\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_legendre(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remarkably, there is a universal rule to choose the $w_k$ and $x_k$:\n",
    "* $x_k = $ roots of $N^{\\text{th}}$ Legendre polynomial $P_N(x)$.\n",
    "* $\\displaystyle w_k = \\left[\\frac{2}{1-x^2}\\left(\\frac{dP_N}{dx}\\right)^{-2}\\right]_{x={x_k}}$, while $P_N(x_k)=0$.\n",
    "* Tables exist to find these values.\n",
    "    Newman mentions Abramowitz and Stegun, whose book was replaced long ago by NIST's Digital Library for Mathematical Functions. \n",
    "    For Gauss quadrature, see https://dlmf.nist.gov/3.5#v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* That there is such a universal rule is beautiful (see Appendix C of textbook; and in a few slides).\n",
    "    In the context of this course, we'll just accept that it works.\n",
    "* Don’t even write your own program to find sample points and weights: use given subroutines. We will have some practice next week on how to find zeros of a function. But for now...\n",
    "* You have \n",
    "    * `gaussxw.py` for integration limits from $-1$ to $+1$,\n",
    "    * `gaussxwab.py` for integration limits from $a$ to $b$.\n",
    "* The calculation of weights and points is expensive. Use `gaussxw.py` if you are going to change the integration bounds repeatedly (and see end of §5.6.1, pp. 167-168, for how to do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# %load gaussxw\n",
    "from pylab import *\n",
    "def gaussxw(N):\n",
    "\n",
    "    # Initial approximation to roots of the Legendre polynomial\n",
    "    a = linspace(3,4*N-1,N)/(4*N+2)\n",
    "    x = cos(pi*a+1/(8*N*N*tan(a)))\n",
    "\n",
    "    # Find roots using Newton's method\n",
    "    epsilon = 1e-15\n",
    "    delta = 1.0\n",
    "    while delta>epsilon:\n",
    "        p0 = ones(N,float)\n",
    "        p1 = copy(x)\n",
    "        for k in range(1,N):\n",
    "            p0,p1 = p1,((2*k+1)*x*p1-k*p0)/(k+1)\n",
    "        dp = (N+1)*(p0-x*p1)/(1-x*x)\n",
    "        dx = p1/dp\n",
    "        x -= dx\n",
    "        delta = max(abs(dx))\n",
    "\n",
    "    # Calculate the weights\n",
    "    w = 2*(N+1)*(N+1)/(N*N*(1-x*x)*dp*dp)\n",
    "\n",
    "    return x,w\n",
    "\n",
    "def gaussxwab(N,a,b):\n",
    "    x,w = gaussxw(N)\n",
    "    return 0.5*(b-a)*x+0.5*(b+a),0.5*(b-a)*w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# manipulate gaussxw\n",
    "N = 4 \n",
    "gaussxw(N)  # change the argument of the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gaussxw(N)[0]  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can reproduce Newman's figure 5.4 with his code \n",
    "def plot_gaussxw(N):\n",
    "    plt.figure(dpi=150)\n",
    "    plt.bar(gaussxw(N)[0], gaussxw(N)[1], width=0.02)\n",
    "    plt.grid()\n",
    "    plt.title(\"For $N = {}$\".format(N))\n",
    "    plt.xlabel('position $x$')\n",
    "    plt.ylabel('weight $w_k$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_gaussxw(50)  # Newman's figure 5.4 is with N=10 and 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pros**\n",
    "* complicated error formula, but in general: approximation error improves by a factor $c/N^2$ when you increase # of sample points by 1!\n",
    "* e.g., going form $N=10$ to $N=11$ sample points improves your estimate by a factor of $\\sim 100$ $\\Rightarrow$ converge very quickly to true value of the integral.\n",
    "\n",
    "**Cons**\n",
    "* only works well if function is reasonably smooth (since sample points are farther apart),\n",
    "* really hard to get an accurate estimate of the error, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example:\n",
    "\n",
    "Compute $\\displaystyle \\int_{-1}^{1} \\left[x^4 + \\sin(x^2)\\right]dx$ using Gaussian quadrature.\n",
    "\n",
    "Let's see what is going on with sympy, Python's symbolic math package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "init_printing()\n",
    "x = symbols('x', real=True)\n",
    "f = x**4 + sin(x**2)\n",
    "plotting.plot(f, (x, -1, 1))  # plotting is part of SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Integrate f\n",
    "integrate(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# from -1 to +1\n",
    "res=integrate(f, (x, -1, +1))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical evaluation\n",
    "N(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, we could do this as long as the function has an analytical expression that SymPy (or Maple, Mathematica, Wolfram Alpha...) knows how to solve.\n",
    "\n",
    "Gaussian quadrature is more reliable for arbitrary functions, let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell for pseudo-code\n",
    "# NOTE HOW I KEEP IT AS A SEPARATE BLOCK OF CODE\n",
    "# Load Newman's functions for gaussxw\n",
    "# Load numpy, matplotlib...\n",
    "# define function to integrate\n",
    "# define N\n",
    "# call gaussxw for xi, wi\n",
    "# initialize integral to 0.\n",
    "# loop over sample points to compute integral\n",
    "# print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell for code\n",
    "# Load Newman's functions for gaussxw\n",
    "# Load numpy\n",
    "import gaussxw as gsx\n",
    "import numpy as np\n",
    "# define function\n",
    "def f(x):\n",
    "    return x**4 + np.sin(x**2)\n",
    "\n",
    "# define N\n",
    "N = 10\n",
    "# call gausswx for xi, wi\n",
    "x, w = gsx.gaussxw(N)\n",
    "\n",
    "# initialize integral to 0.\n",
    "I = 0.\n",
    "# loop over sample points to compute integral\n",
    "for k in range(N):\n",
    "    I += w[k]*f(x[k])\n",
    "# print\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analytical result was 1.0205366034467\n",
    "\n",
    "Not bad, right? Only $N=10$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Derivation outline\n",
    "\n",
    "(*will not cover in class*)\n",
    "\n",
    "Appendix C of Newman:\n",
    "* Use the fact that \n",
    "    $$\\forall k\\in\\{0, \\dots N-1\\}, \\quad \\int_{-1}^1 x^kP_N(x)dx = 0$$\n",
    "    and\n",
    "    $$\\int_{-1}^1 [P_N(x)]^2dx = \\frac{2}{2N+1}.$$\n",
    "* Suppose $f(x) = A_{2N-1} x^{2N-1} + A_{2N-2}x^{2N-2} + \\dots + A_0$ is a decent approximation, and divide by $P_N(x)$:\n",
    "    $$f(x) = q(x)P_N(x) + r(x),$$\n",
    "    with $q$, $r$ polynomials of order $N-1$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Therefore, \n",
    "    $$\\int_{-1}^1 f(x)dx \\approx \\sum_{k=1}^N w_k f(x_k) \\\\\n",
    "    = \\sum_{k=1}^N w_k q(x_k)P_N(x_k) + \\sum_{k=1}^N w_k r(x_k)$$\n",
    "* Choose $x_k$'s such that $P_N(x_k) = 0$ (roots)\n",
    "    $$\\Rightarrow\\ \\int_{-1}^1 f(x)dx \\approx \\sum_{k=1}^N w_k r(x_k),$$\n",
    "    i.e., as accurate as integrating a polynomial of order $N-1$.\n",
    "* No loss of information, because \"encoded\" in the $(x_k, w_k)$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical derivatives\n",
    "\n",
    "* Simpler than numerical integration, in  a way.\n",
    "* Computing errors is usually a doozey though.\n",
    "* Based on Taylor series approximations.\n",
    "* Use Taylor series approximations to estimate errors.\n",
    "\n",
    "\n",
    "1. Forward difference approximation: $\\displaystyle f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$,\n",
    "2. Backward difference approximation: $\\displaystyle f'(x) \\approx \\frac{f(x) - f(x-h)}{h}$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic philosophy for error estimation\n",
    "\n",
    "Use Taylor series to find error in these approximations:\n",
    "$$ f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + h.o.t.$$\n",
    "Isolate for $f'(x)$:\n",
    "$$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}2 f''(x) + h.o.t.$$\n",
    "$\\Rightarrow$ error is 1st-order in $h$ (same is true for backward difference method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Central differences\n",
    "\n",
    "* Using Taylor series to find sneaky improvements to finite difference (FD) schemes.\n",
    "* Example: central FD method:\n",
    "$$f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.$$\n",
    "* Notice it still only involves subtracting 2 points, it's just that the location of the 2 points is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Error:\n",
    "$$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + h.o.t.$$\n",
    "$$f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + h.o.t.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Substract:\n",
    "$$ f(x+h) - f(x+h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + h.o.t.$$\n",
    "* Isolate for $f'(x)$ and add:\n",
    "$$f'(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\boxed{\\frac{h^2}{6}f'''(x)} + h.o.t.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* So we see that this formula is accurate to 2nd order in $h$.\n",
    "* Can get higher-order methods by including more points (see table 5.1 on page 196).\n",
    "* Might have to do different things near the boundaries.\n",
    "* Partial derivatives: similar techniques.\n",
    "* Higher order derivatives (e.g., $f''$): similar techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Roundoff error, a comeback\n",
    "\n",
    "* Let’s take another look at this formula:\n",
    "$$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + h.o.t.$$\n",
    "* What happens when we consider roundoff error? Recall that substracting numbers that are close is dangerous!\n",
    "* Each of the terms $f(x+h)$ and $f(x)$ have error $\\sim C|f(x)|$. Their difference will have approximate error $2Cf(x)$ (worst case scenario).\n",
    "* So in fact there are two sources of error and this leads to (eqn. (5.91) in book):\n",
    "$$\\epsilon = \\underbrace{\\frac{2C|f(x)|}{h}}_{\\text{round-off error}} + \\underbrace{\\frac{1}{2}h |f''(x)| + h.o.t.}_{\\text{approximation error}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Factor $1/h \\Rightarrow$ increasing $N$ could increase the error.\n",
    "* The error has a minimum when $h$ satisfies\n",
    "$$h = \\sqrt{4C\\left|\\frac{f(x)}{f''(x)}\\right|} \\qquad (5.93).$$\n",
    "(*Note: $x$ is not a variable in this discussion. $f(x)$ should be interpreted as the typical magnitude of $f$.*)\n",
    "* Example: $f(x), f''(x) = O(1) \\Rightarrow h\\sim O(\\sqrt{C}) = O(10^{-8})$. In this case, the error is $\\epsilon = O(10^{-8})$, from\n",
    "$$\\epsilon = h|f''(x)| = \\sqrt{4C \\left|f(x)f''(x)\\right|} \\qquad (5.94).$$\n",
    "* There are two points: there's a limit to the improvement you can obtain by going to finer resolution, and the precision expected on differentiation is orders of magnitude less than that of other operations we have discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Integrals\n",
    "\n",
    "### Newton-Cotes formulas\n",
    "\n",
    "* Like Riemann, Trapezoidal and Simpson's rule, but with arbitrary order.\n",
    "* Recall from last week: Euler-MacLaurin and practical estimations of errors work\n",
    "* Your can compute the coefficients for each, but you might as well use tables (chances are, you'll never go beyond Simpson)\n",
    "\n",
    "### Gaussian quadrature\n",
    "\n",
    "* Unlike Newton-Cotes, the points of integration are not equally-spaced\n",
    "* How to find weights and sample points: use routines, know that there are properties of Legendre polynomials under the hood\n",
    "* Every time $N$ increase by $1$, error multiplied by $\\propto 1/N^2$: very accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "## Derivatives\n",
    "\n",
    "* Taylor expansions give formulas **and** error estimates.\n",
    "* Different orders of accuracy as we refine our use of Taylor expansions\n",
    "* Not the only error! As $h$ goes down, approximation error goes down, but machine precision error goes up! (relatively speaking) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
